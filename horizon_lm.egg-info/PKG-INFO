Metadata-Version: 2.4
Name: horizon-lm
Version: 0.1.0
Summary: Horizon-LM: Single-GPU Large Model Training with CPU-backed parameters
Home-page: https://github.com/yourusername/Horizon-LM
Author: Horizon-LM Team
Keywords: deep-learning,large-language-models,training,gpu,cpu-offloading
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: transformers>=4.30.0
Requires-Dist: datasets>=2.0.0
Requires-Dist: psutil>=5.9.0
Requires-Dist: numpy>=1.20.0
Requires-Dist: pyyaml>=6.0
Provides-Extra: flash-attn
Requires-Dist: flash-attn>=2.0.0; extra == "flash-attn"
Provides-Extra: deepspeed
Requires-Dist: deepspeed>=0.10.0; extra == "deepspeed"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: isort>=5.10.0; extra == "dev"
Requires-Dist: flake8>=4.0.0; extra == "dev"
Dynamic: author
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Horizon-LM

**Single-GPU training for large language models (32B+) with CPU-backed parameters.**

Train models like Qwen 2.5 32B, LLaMA 3 70B on a single GPU by storing parameters on CPU and using GPU for computation.

## Quick Start

```bash
# Install
git clone <repository-url>
cd Horizon-LM
pip install -e .

# Train
python examples/train.py --config examples/configs/qwen_32b.yaml
```

## Features

- âœ… **Single GPU Training**: Train 120B+ models on one GPU
- âœ… **CPU-Backed Parameters**: FP32 master weights on CPU, BF16 working copy on GPU
- âœ… **YAML Configuration**: Easy model/dataset/hyperparameter configuration
- âœ… **Flash Attention**: Memory-efficient attention computation
- âœ… **DeepSpeed CPUAdam**: 5-7x faster optimizer
- âœ… **Auto CUDA Extension**: Automatically builds optimized CUDA kernels

## Supported Models

| Model Family | Model Sizes | Status | Template |
|--------------|-------------|--------|----------|
| **Qwen2/Qwen2.5** | 0.5B/1.5B/3B/7B/14B/32B/72B | âœ… | `qwen` |
| Qwen3 | 0.6B/1.7B/4B/8B/14B/32B/80B/235B | ğŸ”„ Pending | `qwen3` |
| Llama 3/3.1/3.2/3.3 | 1B/3B/8B/70B | ğŸ”„ Pending | `llama3` |
| Llama 4 | 109B/402B | ğŸ”„ Pending | `llama4` |
| DeepSeek (LLM/Code/MoE) | 7B/16B/67B/236B | ğŸ”„ Pending | `deepseek` |
| DeepSeek 3 | 236B/671B | ğŸ”„ Pending | `deepseek3` |
| DeepSeek R1 | 1.5B/7B/8B/14B/32B/70B/671B | ğŸ”„ Pending | `deepseekr1` |
| Mistral/Mixtral | 7B/8x7B/8x22B | ğŸ”„ Pending | `mistral` |
| Phi-4 | 3.8B/14B | ğŸ”„ Pending | `phi4` |
| GPT-OSS | 20B/120B | ğŸ”„ Pending | `gpt_oss` |
| GLM-4/GLM-4.5 | 9B/32B/106B/355B | ğŸ”„ Pending | `glm4` |
| InternLM 2/3 | 7B/8B/20B | ğŸ”„ Pending | `intern2` |
| Gemma 2/3 | 2B/7B/9B/27B | ğŸ”„ Pending | `gemma2` |
| Yi | 6B/9B/34B | ğŸ”„ Pending | `yi` |
| Baichuan 2 | 7B/13B | ğŸ”„ Pending | `baichuan2` |
| ChatGLM 3/4 | 6B/9B | ğŸ”„ Pending | `chatglm3` |

**Legend:**
- âœ… Fully supported and tested
- ğŸ”„ Pending - Coming soon

Currently, **Qwen2/Qwen2.5** models are fully supported. Other models will be added progressively.

## Usage

### 1. Configure Training

Edit `examples/configs/qwen_32b.yaml`:

```yaml
model:
  name: "Qwen/Qwen2.5-32B-Instruct"
  dtype: "bfloat16"

dataset:
  path: "/path/to/your/dataset"
  max_seq_len: 1024

training:
  batch_size: 96
  num_steps: 1000
  learning_rate: 1.0e-5

optimizer:
  type: "deepspeed_adam"  # or "adamw"
```

### 2. Train

```bash
# Use config file
python examples/train.py --config examples/configs/qwen_32b.yaml

# Override parameters
python examples/train.py \
    --config examples/configs/qwen_32b.yaml \
    --batch-size 64 \
    --num-steps 500
```

### 3. Use in Python

```python
from infinity import CPUMasterModel, CPUMasterConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load model
config = CPUMasterConfig(
    model_name="Qwen/Qwen2.5-32B-Instruct",
    dataset_path="/path/to/dataset",
    batch_size=96,
)

tokenizer = AutoTokenizer.from_pretrained(config.model_name)
hf_model = AutoModelForCausalLM.from_pretrained(
    config.model_name,
    torch_dtype=torch.bfloat16,
    device_map="cpu"
)

model = CPUMasterModel(hf_model, config)

# Train...
loss, tokens, timing = model.forward_and_backward(
    input_ids, attention_mask, labels
)
```

## Available Configs

| Config | Model | Batch Size | Seq Len |
|--------|-------|------------|---------|
| `qwen_32b.yaml` | Qwen 2.5 32B | 96 | 1024 |
| `qwen_7b.yaml` | Qwen 2.5 7B | 160 | 2048 |

See `examples/configs/README.md` for detailed configuration guide.

## Requirements

- **GPU**: NVIDIA GPU with 40GB+ VRAM (A100, H100, GH200)
- **CPU RAM**: 256GB+ for 32B models
- **CUDA**: 11.8+
- **PyTorch**: 2.0+
- **Python**: 3.9+

## How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CPU Memory (FP32 Master Params)  â”‚  â† 128GB for 32B model
â”‚   + Adam Moments (m, v)            â”‚  â† 128GB for optimizer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†• Async Transfer
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPU Memory (BF16 Working Copy)   â”‚  â† 8-180GB
â”‚   + Activations + Gradients        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Techniques:**
- Double buffering for overlapped weight transfer
- Gradient checkpointing every K layers
- Async gradient collection with slab pool
- Manual gradient computation (no autograd overhead)

## Performance

Training Qwen 2.5 32B on single H100:
- **Memory**:  ~415GB CPU
- **Throughput**: ~259 TFLOPS
- **Batch Size**: 96 (seq_len=1024)

## Installation Details

When you run `pip install -e .`:
1. Installs Python dependencies (PyTorch, Transformers, etc.)
2. Builds CUDA extension automatically (if CUDA available)
3. Sets up `infinity` library for import

Optional dependencies:
```bash
# Flash Attention (recommended)
pip install flash-attn

# DeepSpeed CPUAdam (5-7x faster optimizer)
pip install deepspeed
```

## Troubleshooting

**Out of Memory?**
- Reduce `batch_size` in config
- Increase `checkpoint_interval`
- Reduce `max_seq_len`

**Slow Training?**
- Use `deepspeed_adam` optimizer
- Install Flash Attention
- Increase `num_workers` for data loading

**CUDA Extension Failed?**
- Training still works without it (slightly slower)
- Check CUDA version: `nvcc --version`
- Manually build: `cd infinity/cuda_pipeline && python setup.py install`

## Citation

If you use Horizon-LM in your research, please cite:

```bibtex
@software{horizon-lm,
  title = {Horizon-LM: Single-GPU Large Model Training},
  author = {Horizon-LM Team},
  year = {2024},
  url = {https://github.com/yourusername/Horizon-LM}
}
```

